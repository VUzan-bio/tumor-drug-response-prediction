"""
Lightweight baseline training using scikit-learn on processed data and split CSVs.

Steps:
1) Load processed tables: omics.parquet, drug_fingerprints.parquet, labels.parquet.
2) Load a split CSV produced by scripts/make_splits.py with columns
   [cell_line, drug, ln_ic50, split].
3) Fit PCA transformers on omics and drug fingerprints using the train split only.
4) Train an MLP regressor on the concatenated PCA features.
5) Report RMSE and Pearson correlation on train/val/test splits.

Usage:
    python scripts/train_baseline_ml.py \
        --processed-dir data/processed \
        --split-csv data/processed/splits/random_pair_split.csv \
        --outdir outputs/baseline_random

You can re-run with different split CSVs (cellline_holdout_split.csv, tissue_holdout_split.csv)
to test generalization.
"""

# NOTE: On Windows, this baseline may require MKL BLAS (conda-forge
# `libblas=*=*mkl` and `mkl`) to avoid OpenBLAS crashes during PCA/SVD.
# If the interpreter exits with no traceback, run `python scripts/test_linalg.py`.

from __future__ import annotations

import argparse
from pathlib import Path
import sys
from typing import Dict

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

ROOT = Path(__file__).resolve().parents[1]
SRC = ROOT / "src"
if str(SRC) not in sys.path:
    sys.path.append(str(SRC))

from tdrp.utils.seed import set_seed


def pearsonr(x: np.ndarray, y: np.ndarray) -> float:
    if x.size == 0:
        return np.nan
    vx = x - x.mean()
    vy = y - y.mean()
    denom = np.sqrt((vx ** 2).sum()) * np.sqrt((vy ** 2).sum())
    if denom == 0:
        return np.nan
    return float((vx * vy).sum() / denom)


def load_processed(processed_dir: Path) -> Dict[str, pd.DataFrame]:
    omics = pd.read_parquet(processed_dir / "omics.parquet")
    drugs = pd.read_parquet(processed_dir / "drug_fingerprints.parquet")
    labels = pd.read_parquet(processed_dir / "labels.parquet")[["cell_line", "drug", "ln_ic50"]]
    return {"omics": omics, "drugs": drugs, "labels": labels}


def build_features(
    omics_df: pd.DataFrame,
    drug_df: pd.DataFrame,
    split_df: pd.DataFrame,
    pca_omics: PCA,
    pca_drug: PCA,
    scaler_omics: StandardScaler,
    scaler_drug: StandardScaler,
) -> (np.ndarray, np.ndarray):
    """Create concatenated PCA features for a subset defined by split_df."""
    cells = split_df["cell_line"].values
    drugs = split_df["drug"].values
    y = split_df["ln_ic50"].values.astype(np.float32)

    omics_mat = omics_df.set_index("cell_line").loc[cells].to_numpy(dtype=np.float32)
    drug_mat = drug_df.set_index("drug").loc[drugs].to_numpy(dtype=np.float32)

    omics_z = scaler_omics.transform(omics_mat)
    drug_z = scaler_drug.transform(drug_mat)

    omics_p = pca_omics.transform(omics_z)
    drug_p = pca_drug.transform(drug_z)

    X = np.concatenate([omics_p, drug_p], axis=1).astype(np.float32)
    return X, y


def train_one_split(
    processed_dir: Path,
    split_csv: Path,
    outdir: Path,
    n_omics_pca: int,
    n_drug_pca: int,
    hidden_layers: tuple[int, ...],
    seed: int,
):
    outdir.mkdir(parents=True, exist_ok=True)
    print("load_processed", flush=True)
    tables = load_processed(processed_dir)
    split_df = pd.read_csv(split_csv)

    # Harmonize key types
    tables["omics"]["cell_line"] = tables["omics"]["cell_line"].astype(str)
    tables["drugs"]["drug"] = tables["drugs"]["drug"].astype(str)
    for col in ["cell_line", "drug"]:
        split_df[col] = split_df[col].astype(str)

    train_df = split_df[split_df["split"] == "train"]
    val_df = split_df[split_df["split"] == "val"]
    test_df = split_df[split_df["split"] == "test"]

    print("fit_scalers", flush=True)
    omics_mat_train = tables["omics"].set_index("cell_line").loc[train_df["cell_line"]].to_numpy(dtype=np.float32)
    drug_mat_train = tables["drugs"].set_index("drug").loc[train_df["drug"]].to_numpy(dtype=np.float32)

    scaler_omics = StandardScaler().fit(omics_mat_train)
    scaler_drug = StandardScaler().fit(drug_mat_train)

    print("fit_pca", flush=True)
    pca_omics = PCA(n_components=min(n_omics_pca, omics_mat_train.shape[1]), random_state=seed).fit(
        scaler_omics.transform(omics_mat_train)
    )
    pca_drug = PCA(n_components=min(n_drug_pca, drug_mat_train.shape[1]), random_state=seed).fit(
        scaler_drug.transform(drug_mat_train)
    )

    print("build_features", flush=True)
    X_train, y_train = build_features(
        tables["omics"], tables["drugs"], train_df, pca_omics, pca_drug, scaler_omics, scaler_drug
    )
    X_val, y_val = build_features(
        tables["omics"], tables["drugs"], val_df, pca_omics, pca_drug, scaler_omics, scaler_drug
    )
    X_test, y_test = build_features(
        tables["omics"], tables["drugs"], test_df, pca_omics, pca_drug, scaler_omics, scaler_drug
    )

    print("fit_mlp", flush=True)
    mlp = MLPRegressor(
        hidden_layer_sizes=hidden_layers,
        activation="relu",
        solver="adam",
        learning_rate_init=1e-3,
        alpha=1e-4,
        max_iter=50,
        random_state=seed,
        early_stopping=True,
        n_iter_no_change=5,
        verbose=False,
    )
    mlp.fit(X_train, y_train)

    def eval_split(name: str, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        preds = mlp.predict(X)
        rmse = float(np.sqrt(np.mean((y - preds) ** 2)))
        r = pearsonr(y, preds)
        return {"rmse": rmse, "pearson": r, "y": y, "pred": preds}

    metrics = {
        "train": eval_split("train", X_train, y_train),
        "val": eval_split("val", X_val, y_val),
        "test": eval_split("test", X_test, y_test),
        "n_train": int(len(train_df)),
        "n_val": int(len(val_df)),
        "n_test": int(len(test_df)),
        "omics_pca": int(pca_omics.n_components_),
        "drug_pca": int(pca_drug.n_components_),
        "hidden_layers": list(hidden_layers),
        "seed": int(seed),
    }

    pd.DataFrame([metrics]).to_json(outdir / "metrics.json", orient="records", lines=True)
    print(f"Split: {split_csv.name}")
    for split in ["train", "val", "test"]:
        m = metrics[split]
        print(f"{split}: RMSE={m['rmse']:.3f} Pearson={m['pearson']:.3f}")

    # Save predictions with identifiers for downstream plots.
    for split_name, df_split, eval_res in [
        ("train", train_df, metrics["train"]),
        ("val", val_df, metrics["val"]),
        ("test", test_df, metrics["test"]),
    ]:
        out_csv = outdir / f"preds_{split_name}.csv"
        preds_df = df_split.copy()
        preds_df["pred"] = eval_res["pred"]
        preds_df.to_csv(out_csv, index=False)

    # Plot predicted vs true for test
    y_true = metrics["test"]["y"]
    y_pred = metrics["test"]["pred"]
    plt.figure(figsize=(6, 6))
    plt.scatter(y_true, y_pred, alpha=0.3, s=10)
    lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]
    plt.plot(lims, lims, "r--", label="y=x")
    plt.xlabel("True ln(IC50)")
    plt.ylabel("Pred ln(IC50)")
    plt.title(f"Pred vs True ({split_csv.name})")
    plt.legend()
    plt.tight_layout()
    plt.savefig(outdir / "pred_vs_true_test.png", dpi=200)
    plt.close()

    # Residual histogram
    resid = y_true - y_pred
    plt.figure(figsize=(6, 4))
    plt.hist(resid, bins=40, color="steelblue", alpha=0.8)
    plt.xlabel("Residual (true - pred)")
    plt.ylabel("Count")
    plt.title(f"Residuals ({split_csv.name})")
    plt.tight_layout()
    plt.savefig(outdir / "residuals_test.png", dpi=200)
    plt.close()

    # Per-drug/per-tissue bars (top 15 by count)
    def per_group(df: pd.DataFrame, group_col: str, prefix: str):
        grp = df.groupby(group_col)
        stats = []
        for name, g in grp:
            if len(g) < 5:
                continue
            y = g["ln_ic50"].values
            p = g["pred"].values
            stats.append(
                {
                    group_col: name,
                    "n": len(g),
                    "rmse": float(np.sqrt(np.mean((y - p) ** 2))),
                    "pearson": pearsonr(y, p),
                }
            )
        if not stats:
            return
        df_stats = pd.DataFrame(stats).sort_values("pearson", ascending=False).head(15)
        plt.figure(figsize=(8, 4))
        plt.bar(df_stats[group_col].astype(str), df_stats["pearson"], color="teal")
        plt.xticks(rotation=60, ha="right")
        plt.ylabel("Pearson r")
        plt.title(f"{prefix} per-{group_col} (top 15 by r)")
        plt.tight_layout()
        plt.savefig(outdir / f"{prefix}_per_{group_col}.png", dpi=200)
        plt.close()

    test_preds = pd.read_csv(outdir / "preds_test.csv")
    per_group(test_preds, "drug", "test")
    if "tissue" in test_preds.columns:
        per_group(test_preds, "tissue", "test")

    return metrics


def main():
    ap = argparse.ArgumentParser(description="Train a simple PCA+MLP baseline on a split CSV.")
    ap.add_argument("--processed-dir", default="data/processed", help="Directory with parquet files.")
    ap.add_argument("--split-csv", required=True, help="Path to split CSV (random_pair_split.csv, etc.)")
    ap.add_argument("--outdir", required=True, help="Output directory for metrics.json")
    ap.add_argument("--omics-pca", type=int, default=256, help="PCA components for omics.")
    ap.add_argument("--drug-pca", type=int, default=128, help="PCA components for drug fingerprints.")
    ap.add_argument(
        "--hidden-layers",
        type=str,
        default="512,256",
        help="Comma-separated hidden layer sizes for the MLP.",
    )
    ap.add_argument("--seed", type=int, default=42, help="Random seed.")
    args = ap.parse_args()

    hidden = tuple(int(x) for x in args.hidden_layers.split(",") if x)
    set_seed(args.seed)

    train_one_split(
        processed_dir=Path(args.processed_dir),
        split_csv=Path(args.split_csv),
        outdir=Path(args.outdir),
        n_omics_pca=args.omics_pca,
        n_drug_pca=args.drug_pca,
        hidden_layers=hidden,
        seed=args.seed,
    )


if __name__ == "__main__":
    main()
